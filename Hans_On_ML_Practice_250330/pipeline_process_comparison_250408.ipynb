{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "iris = datasets.load_iris() #iris 데이터를 불러옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iris 데이터 셑을 학습용과 검증용 데이터 셑으로 나눔\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2222)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 과정 : 파이프라인을 사용하지 않는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.9555555555555556\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.88      1.00      0.94        15\n",
      "           2       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.96      0.96        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#표준정규분포로 스케일링\n",
    "sc = StandardScaler()\n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)\n",
    "\n",
    "svc = SVC(kernel='poly', C = 3, degree = 3).fit(X_train_sc, y_train) #SVM모델로 모델링\n",
    "y_pred = svc.predict(X_test_sc) #예측\n",
    "\n",
    "print(\"정확도\", accuracy_score(y_test, y_pred))\n",
    "print(\"classification report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이프라인을 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도 0.9555555555555556\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        13\n",
      "           1       0.88      1.00      0.94        15\n",
      "           2       1.00      0.88      0.94        17\n",
      "\n",
      "    accuracy                           0.96        45\n",
      "   macro avg       0.96      0.96      0.96        45\n",
      "weighted avg       0.96      0.96      0.96        45\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', SVC(kernel='poly', C = 3, degree = 3))])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "\n",
    "print(\"정확도\", accuracy_score(y_test, y_pred))\n",
    "print(\"classification report\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 적용 유무에 따른 결과의 차이는 없음(당연히)\n",
    "# 파이프라인을 사용하면, 머신러닝하는 여러 단계를 하나로 연결할 수 있어, 편리하게 사용가능\n",
    "# 더 중요한 이유는? : 대형 데이터베이스를 사용하는 경우가 아닐까? (250408)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://step-by-step-digital.tistory.com/148\n",
    "\n",
    "# 데이터셋을 받았을 때, EDA를 한 후 어떤 데이터가 어떤 타입인지 확인을 하고 그에 맞는 변환 처리를 해줍니다. \n",
    "# 결측치처리, outlier처리, 스케일링, 원핫인코딩, 라벨인코딩 등의 과정을 거쳐야합니다. \n",
    "# 데이터의 특성과 목적에 맞게 변환을 해주어야 올바른 모델 성능이 나옵니다. \n",
    "# 그리고 보통의 데이터들은 문자 숫자가 섞여있고 깔끔하게 정제되어 있는 경우가 거의 없습니다. 그렇기 때문에 받을 때마다 전처리를 해야합니다. \n",
    "# 또한 모델 구축할 때 알맞은 파라미터를 찾기 위해 그리드 서치 기법을 사용하면서 모델 구축 및 하이퍼파라미터 튜닝까지 파이프라인에 태울 수 있습니다. \n",
    " \n",
    "# Pipeline은 데이터 전처리 및 모델 구축 과정에서 여러 단계를 순차적으로 처리하는 데 사용되는 유용한 도구입니다. \n",
    "# 파이프라인을 사용하면 좋은 점은 코드가 간결해지며, \n",
    "# 다른 데이터셋이나 환경에서 재사용성이 가능해지며, \n",
    "# 반복적인 작업을 줄일 수 있어 생산성이 향상됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 타입별로 어떤 전처리 방식을 사용해야하는지 알아야함\n",
    "# 사용자 정의 함수도 파이프라인에 넣을 수 있음\n",
    "# 전처리 과정을 한 번에 파이프라인으로 구축할 수 있고\n",
    "# 모델 생성과 튜닝까지도 파이프라인에 넣을 수 있음\n",
    "\n",
    "# 원-핫 인코딩할 때, handle_unknown 처리해야함\n",
    "# -> 그래야 학습 데이터 이후 새로운 데이터가 들어갈 때, 0으로 변환되어 데이터 값이 변하지 않음\n",
    "\n",
    "\n",
    "# pipeline을 사용하여 전처리를 하면 inverse_transform을 사용할 수 없음\n",
    "# -> FunctionTransform을 사용한다.\n",
    "\n",
    "'''\n",
    "# FunctionTransform: inverse_func 적용\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
